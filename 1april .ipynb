{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1e3ee4-10da-4fdd-80dd-df5eeb795db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63172b71-d042-4812-918b-41165b9096f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression \n",
    "Linear regression is used when the dependent variable is continuous and the independent variables are also continuous or categorical.\n",
    "The objective of linear regression is to find a linear relationship between the dependent variable and the independent variables, which can be used to make predictions on the value of the dependent variable based on the values of the independent variables.\n",
    "\n",
    "A common example of linear regression is predicting the salary of an employee based on their years of experience.\n",
    "\n",
    "# logistic regression \n",
    " logistic regression is used when the dependent variable is categorical, that is, it takes on one of two possible values (binary classification), such as \"yes\" or \"no,\" \"true\" or \"false,\" or \"spam\" or \"not spam.\"\n",
    "    The independent variables can be either continuous or categorical.\n",
    "    The objective of logistic regression is to find the probability of an event occurring given the values of the independent variables. \n",
    "    \n",
    "    An example of logistic regression would be predicting whether a person will default on a loan based on their credit score, income, and other relevant factors.\n",
    "    \n",
    "    A scenario where logistic regression would be more appropriate than linear regression is when the dependent variable is categorical, as mentioned above.\n",
    "    For example, if we want to predict whether a customer will buy a product or not based on their age, gender, income, and other relevant factors, we would use logistic regression.\n",
    "    In this case, the dependent variable is categorical (buy or not buy), so linear regression would not be appropriate. \n",
    "    Logistic regression would be better suited for this scenario because it can model the probability of the event occurring, rather than predicting a continuous value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d349ab5-5d1b-4a67-9686-1e39be8783c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab05125-836e-422e-b5d5-957271aac201",
   "metadata": {},
   "outputs": [],
   "source": [
    " the cost function used is the logistic loss or cross-entropy loss. It measures the difference between the predicted probabilities of the model and the true labels of the training data.\n",
    "\n",
    "The logistic loss function is defined as follows:\n",
    "\n",
    "J(θ) = -1/m * Σ [y(i) * log(hθ(x(i))) + (1 - y(i)) * log(1 - hθ(x(i)))]\n",
    "\n",
    "where:\n",
    "\n",
    "J(θ) is the cost function\n",
    "m is the number of training examples\n",
    "x(i) is the i-th training example\n",
    "y(i) is the true label of the i-th training example (0 or 1)\n",
    "hθ(x(i)) is the predicted probability that y(i) = 1 given x(i) and the model parameters θ\n",
    "The logistic loss function penalizes the model more heavily for incorrect predictions and puts less emphasis on correct predictions.\n",
    "\n",
    "To optimize the cost function in logistic regression, we use an algorithm called gradient descent.\n",
    "The goal of gradient descent is to find the values of the model parameters θ that minimize the cost function J(θ). \n",
    "At each iteration of the algorithm, we update the parameters θ by taking a step in the direction of the negative gradient of the cost function. \n",
    "The size of the step is controlled by a parameter called the learning rate.\n",
    "\n",
    "The update rule for gradient descent in logistic regression is:\n",
    "\n",
    "θ = θ - α * ∇J(θ)\n",
    "\n",
    "where:\n",
    "\n",
    "α is the learning rate\n",
    "∇J(θ) is the gradient of the cost function J(θ) with respect to θ\n",
    "We repeat this process until the cost function converges or reaches a minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defbaf2a-e406-4e2e-92af-15be06b4545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13ae7a-1029-44b0-bbe8-78a315d92736",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting, which occurs when a model learns the noise in the training data instead of the underlying patterns. \n",
    "Overfitting leads to poor performance on new, unseen data.\n",
    "\n",
    "In logistic regression, regularization involves adding a penalty term to the cost function that encourages the model to have smaller parameter values.\n",
    "This penalty term controls the complexity of the model and prevents it from fitting the noise in the data.\n",
    "There are two common types of regularization: L1 regularization (also known as Lasso regularization) and L2 regularization (also known as Ridge regularization).\n",
    "\n",
    "L1 regularization adds a penalty term to the cost function proportional to the absolute value of the model parameters:\n",
    "\n",
    "J(θ) = -1/m * Σ [y(i) * log(hθ(x(i))) + (1 - y(i)) * log(1 - hθ(x(i)))] + λ * Σ |θ|\n",
    "\n",
    "where λ is the regularization parameter that controls the strength of the penalty term.\n",
    "\n",
    "L2 regularization adds a penalty term to the cost function proportional to the square of the model parameters:\n",
    "\n",
    "J(θ) = -1/m * Σ [y(i) * log(hθ(x(i))) + (1 - y(i)) * log(1 - hθ(x(i)))] + λ/2 * Σ θ²\n",
    "\n",
    "where λ/2 is the regularization parameter that controls the strength of the penalty term.\n",
    "\n",
    "The effect of regularization is to shrink the model parameters towards zero, which reduces the complexity of the model and helps prevent overfitting.\n",
    "The choice between L1 and L2 regularization depends on the specific problem and the properties of the data.\n",
    "\n",
    "regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function that encourages the model to have smaller parameter values. \n",
    "It helps to control the complexity of the model and improve its generalization performance on new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bcd7c6-1993-47b3-9932-13bf0f11bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec21c93-9a8b-42b0-b3c9-a2218c7f9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier, such as a logistic regression model. \n",
    "It is used to evaluate the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for different classification thresholds.\n",
    "\n",
    "The true positive rate (TPR) is the proportion of positive examples  that are correctly classified as positive by the model, while the false positive rate (FPR) is the proportion of negative examples  that are incorrectly classified as positive by the model.\n",
    "\n",
    "To construct the ROC curve, we plot the TPR on the y-axis against the FPR on the x-axis for different classification thresholds. Each point on the curve represents a different trade-off between TPR and FPR.\n",
    "The diagonal line on the plot represents a random classifier.\n",
    "\n",
    "A good classifier should have a high TPR and a low FPR, which corresponds to a point in the top-left corner of the plot.\n",
    "The area under the ROC curve (AUC) is a commonly used metric to evaluate the performance of a classifier.\n",
    "A perfect classifier has an AUC of 1, while a random classifier has an AUC of 0.5.\n",
    "\n",
    "To evaluate the performance of a logistic regression model using the ROC curve, we first calculate the predicted probabilities of the model for the test data.\n",
    "We then use these probabilities to classify the examples using different classification thresholds.\n",
    "We can then plot the ROC curve and calculate the AUC to determine the performance of the model. A higher AUC indicates better performance.\n",
    "\n",
    "the ROC curve is a graphical representation of the performance of a binary classifier, such as a logistic regression model, based on the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for different classification thresholds.\n",
    "It is used to evaluate the performance of the model and determine the optimal classification threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16553659-4aea-4dcf-ba36-4eb2e4603005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c0de0-00fb-4d81-be41-c28260695a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Univariate feature selection: This technique selects the most significant features based on their individual relationship with the target variable.\n",
    "It calculates a statistical metric, such as chi-squared or ANOVA, for each feature and selects the top k features based on their score.\n",
    "\n",
    "Recursive feature elimination: This technique recursively removes the least important features from the model until a predefined number of features is reached.\n",
    "It evaluates the model performance after each feature is removed and selects the best subset of features that results in the highest performance.\n",
    "\n",
    "L1 regularization: As mentioned earlier, L1 regularization adds a penalty term proportional to the absolute value of the model parameters. \n",
    "This penalty encourages the model to have smaller parameter values, effectively shrinking some of the parameters to zero, and eliminating some features from the model.\n",
    "\n",
    "Principal component analysis (PCA): This technique transforms the original features into a new set of orthogonal features that capture the most significant information in the data. \n",
    "It then selects a subset of the most important principal components to be used as input in the logistic regression model.\n",
    "\n",
    "These techniques help improve the performance of the logistic regression model by reducing the complexity of the model, eliminating irrelevant or redundant features, and improving its ability to generalize to new data.\n",
    "By selecting the most relevant features, the model can learn the underlying patterns in the data more accurately and avoid overfitting.\n",
    "However, the choice of feature selection technique depends on the specific problem and the properties of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75ab127-1ec7-4994-bb8d-a512de8c3197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c88b96-aa53-4272-a33f-22daead03c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resampling techniques: This involves either oversampling the minority class or undersampling the majority class to balance the class distribution in the training data. \n",
    "Oversampling can be done by duplicating existing examples or generating new synthetic examples using techniques such as SMOTE . \n",
    "Undersampling involves randomly removing examples from the majority class to match the number of examples in the minority class.\n",
    "\n",
    "Cost-sensitive learning: This involves assigning different misclassification costs to the different classes based on their relative importance. \n",
    "In logistic regression, this can be achieved by adjusting the weights of the positive and negative examples in the cost function to reflect the class distribution.\n",
    "\n",
    "Ensemble methods: This involves combining multiple logistic regression models trained on different subsets of the data to improve the classification performance. \n",
    "One example is the Bagging algorithm, which trains multiple logistic regression models on random subsets of the data and combines their predictions to make a final decision.\n",
    "\n",
    "Threshold adjustment: This involves adjusting the classification threshold of the logistic regression model to balance the trade-off between the true positive rate and false positive rate. \n",
    "For imbalanced datasets, it is often preferable to use a lower threshold that increases the sensitivity of the model to the minority class.\n",
    "\n",
    "Using different performance metrics: Instead of using the traditional accuracy metric, it is often more appropriate to use metrics such as precision, recall, F1 score, or AUC that take into account the class distribution and provide a more comprehensive evaluation of the model's performance on imbalanced datasets.\n",
    "\n",
    "Overall, handling class imbalance in logistic regression requires a combination of these strategies, depending on the specific problem and the properties of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acc24bd-4eaa-4f6a-9605-dc15e8283891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45070d-bab2-4277-b09b-12aecc98daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity: This occurs when two or more independent variables are highly correlated with each other. It can lead to unstable estimates of the coefficients and make it difficult to interpret the contribution of each variable to the model. One way to address multicollinearity is to remove one of the correlated variables or combine them into a single variable. Another approach is to use regularization techniques, such as L1 or L2 regularization, that penalize large coefficients and encourage the model to select only the most relevant variables.\n",
    "\n",
    "Outliers: Outliers are data points that are significantly different from the rest of the data and can have a disproportionate impact on the model. One way to address outliers is to remove them from the dataset or transform them using methods such as winsorization or log transformation.\n",
    "\n",
    "Missing data: Missing data can lead to biased estimates and reduce the accuracy of the model. One approach to handling missing data is to remove the observations with missing values. Another approach is to impute the missing values using methods such as mean imputation, regression imputation, or multiple imputation.\n",
    "\n",
    "Nonlinear relationships: Logistic regression assumes a linear relationship between the independent variables and the log odds of the outcome variable. If there is evidence of a nonlinear relationship, such as a U-shaped or inverted U-shaped curve, a polynomial or spline transformation can be used to model the nonlinearity.\n",
    "\n",
    "Overfitting: Overfitting occurs when the model is too complex and captures noise and idiosyncrasies in the training data rather than the underlying patterns. Regularization techniques, such as L1 or L2 regularization, can be used to constrain the model complexity and prevent overfitting.\n",
    "\n",
    "Imbalanced data: As discussed in the previous question, imbalanced data can lead to biased model predictions and poor performance on the minority class. Strategies such as resampling, cost-sensitive learning, and threshold adjustment can be used to address class imbalance.\n",
    "\n",
    "addressing these issues and challenges in logistic regression requires a combination of statistical techniques, data preprocessing, and careful modeling decisions. It is important to carefully evaluate the data and the model assumptions and to select the most appropriate methods based on the specific problem and the properties of the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
